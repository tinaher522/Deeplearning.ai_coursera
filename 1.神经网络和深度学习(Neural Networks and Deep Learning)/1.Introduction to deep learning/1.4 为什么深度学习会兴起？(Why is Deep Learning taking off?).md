### 1.4 为什么深度学习会兴起？(Why is Deep Learning taking off?)

推动深度学习变得如此热门的主要因素包括数据规模、计算量及算法的创新。

为什么深度学习能够如此有效?因为传统机器学习算法在处理规模巨大的数据时，它的性能一开始在增加更多数据时会上升，但是一段变化后它的性能就会像一个高原一样。**传统机器学习算法无法高效利用大数据的优势。**现在数字化社会来临，每天都会有海量的数据，尤其近年来物联网的发展也很迅猛，这个社会的每一个摄像头每一个传感器，每一天都在产生大量数据。

神经网络展现出的是，如果你训练一个小型的神经网络，那么这个性能可能会像下图黄色曲线表示那样；如果你训练一个稍微大一点的神经网络，比如说一个中等规模的神经网络（下图蓝色曲线），它在某些数据上面的性能也会更好一些；如果你训练一个非常大的神经网络，它就会变成下图绿色曲线那样，并且保持变得越来越好。因此可以注意到两点：如果你想要获得较高的性能体现，那么你有两个条件要完成，第一个是你需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点，另外你需要能画到 x 轴的这个位置，所以你需要很多的数据。因此我们经常说规模一直在推动深度学习的进步，这里的规模指的也同时是神经网络的规模，我们需要一个带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是**要么训练一个更大的神经网络，要么投入更多的数据**，这只能在一定程度上起作用，因为最终你耗尽了数据，或者最终你的网络是如此大规模导致将要用太久的时间去训练，但是仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。为了使这个图更加从技术上讲更精确一点，我在x轴下面已经写明的数据量，这儿加上一个标签（label）量，通过添加这个标签量，也就是指在训练样本时，我们同时输入 x 和标签 y ，接下来引入一点符号，使用小写的字母 m 表示训练集的规模，或者说训练样本的数量，这个小写字母 m 就横轴结合其他一些细节到这个图像中。

![](./images/7.png)

**在这个小的训练集中，各种算法的优先级事实上定义的也不是很明确**，所以如果因此你知道在这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用工程选择特征方面的能力以及算法处理方面的一些细节，只是在某些大数据规模非常庞大的训练集，也就是在右边这个 m 会非常的大时，我们能更加持续地看到更大的由神经网络控制的其它方法没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。假设有些人训练出了一个**SVM**（支持向量机）表现的更接近正确特征，然而有些人训练的规模大一些，可能在这个小的训练集中**SVM**算法可以做的更好。因此在这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用工程选择特征方面的能力以及算法处理方面的一些细节，只是在某些大数据规模非常庞大的训练集，也就是在右边这个 m 会非常的大时，我们能更加持续地看到更大的由神经网络控制的其它方法。


在**深度学习萌芽的初期**，数据的规模以及计算量，局限在我们对于训练一个特别大的神经网络的能力。而现在随着CPU、GPU硬件运行速度的提升，训练大型神经网络变得可行。Moreover, 这几年，算法方面也有极大的创新。许多**算法方面的创新**，一直是在尝试着使得神经网络运行的更快。

作为一个具体的例子，神经网络方面的一个巨大突破是从**sigmoid**函数转换到一个**ReLU**函数，如下图2.
![](./images/8.png)
一个使用**sigmoid**函数和机器学习问题是，在这个区域，也就是这个**sigmoid**函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做**ReLU**的函数（修正线性单元），**ReLU**它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将**Sigmod**函数转换成**ReLU**函数，便能够使得一个叫做梯度下降（**gradient descent**）的算法运行的更快，这就是一个或许相对比较简单的算法创新的例子。